{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76b19cc-fae5-48b5-936d-f3a83e8803d0",
   "metadata": {},
   "source": [
    "# acell.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed5ced1-8916-450e-8734-8fde10c6cee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 01:42:34.673062: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-30 01:42:35.467635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jun 10 17:10:22 2019\n",
    "\n",
    "@author: dhh\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from layer_assist import Unit\n",
    "#from Unit import call\n",
    "import tensorflow as tf\n",
    "dim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6684756-3389-474b-9dfd-cdfe7d5a2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_assist_data():\n",
    "    sz_adj = pd.read_csv('../DAT/sz_adj.csv', header=None)\n",
    "    adj = np.mat(sz_adj)\n",
    "    data = pd.read_csv('../DAT/sz_speed.csv')\n",
    "    return data, adj\n",
    "\n",
    "data, adj = load_assist_data()\n",
    "time_len = data.shape[0]\n",
    "num_nodes = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37e47d6-acab-4a32-bd51-028172a67dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data1,  time_len, train_rate, seq_len, pre_len, model_name,scheme):\n",
    "    train_size = int(time_len * train_rate)\n",
    "    train_data = data1[0:train_size]\n",
    "    test_data = data1[train_size:time_len]            \n",
    "            \n",
    "    if model_name == 'tgcn':################TGCN###########################\n",
    "        trainX, trainY, testX, testY = [], [], [], []\n",
    "        for i in range(len(train_data) - seq_len - pre_len):\n",
    "            a1 = train_data[i: i + seq_len + pre_len]\n",
    "            trainX.append(a1[0 : seq_len])\n",
    "            trainY.append(a1[seq_len : seq_len + pre_len])\n",
    "        for i in range(len(test_data) - seq_len -pre_len):\n",
    "            b1 = test_data[i: i + seq_len + pre_len]\n",
    "            testX.append(b1[0 : seq_len])\n",
    "            testY.append(b1[seq_len : seq_len + pre_len])         \n",
    "            \n",
    "    else:################AST-GCN###########################\n",
    "        sz_poi = pd.read_csv('../DAT/sz_poi.csv',header = None)\n",
    "        sz_poi = np.transpose(sz_poi)\n",
    "        sz_poi_max = np.max(np.max(sz_poi))\n",
    "        sz_poi_nor = sz_poi/sz_poi_max\n",
    "        sz_weather = pd.read_csv('../DAT/sz_weather.csv',header = None)\n",
    "        sz_weather = np.mat(sz_weather)\n",
    "        sz_weather_max = np.max(np.max(sz_weather))\n",
    "        sz_weather_nor = sz_weather/sz_weather_max\n",
    "        sz_weather_nor_train = sz_weather_nor[0:train_size]\n",
    "        sz_weather_nor_test = sz_weather_nor[train_size:time_len]\n",
    "\n",
    "        if J == 1:#add poi(dim+1)\n",
    "            trainX, trainY, testX, testY = [], [], [], []\n",
    "            for i in range(len(train_data) - seq_len - pre_len):\n",
    "                a1 = train_data[i: i + seq_len + pre_len]\n",
    "                a = np.row_stack((a1[0:seq_len],sz_poi_nor[:1]))\n",
    "                trainX.append(a)\n",
    "                trainY.append(a1[seq_len : seq_len + pre_len])\n",
    "            for i in range(len(test_data) - seq_len -pre_len):\n",
    "                b1 = test_data[i: i + seq_len + pre_len]\n",
    "                b = np.row_stack((b1[0:seq_len],sz_poi_nor[:1]))\n",
    "                testX.append(b)\n",
    "                testY.append(b1[seq_len : seq_len + pre_len])\n",
    "        if J == 2:#add weather(dim+11)\n",
    "            trainX, trainY, testX, testY = [], [], [], []\n",
    "            for i in range(len(train_data) - seq_len - pre_len):\n",
    "                a1 = train_data[i: i + seq_len + pre_len]\n",
    "                a2 = sz_weather_nor_train[i: i + seq_len + pre_len]\n",
    "                a = np.row_stack((a1[0:seq_len],a2[0: seq_len + pre_len]))\n",
    "                trainX.append(a)\n",
    "                trainY.append(a1[seq_len : seq_len + pre_len])\n",
    "            for i in range(len(test_data) - seq_len -pre_len):\n",
    "                b1 = test_data[i: i + seq_len + pre_len]\n",
    "                b2 = sz_weather_nor_test[i: i + seq_len + pre_len]\n",
    "                b = np.row_stack((b1[0:seq_len],b2[0: seq_len + pre_len]))\n",
    "                testX.append(b)\n",
    "                testY.append(b1[seq_len : seq_len + pre_len])\n",
    "        else:#add kg(dim+12)\n",
    "            trainX, trainY, testX, testY = [], [], [], []\n",
    "            for i in range(len(train_data) - seq_len - pre_len):\n",
    "                a1 = train_data[i: i + seq_len + pre_len]\n",
    "                a2 = sz_weather_nor_train[i: i + seq_len + pre_len]\n",
    "                a = np.row_stack((a1[0:seq_len],a2[0: seq_len + pre_len],sz_poi_nor[:1]))\n",
    "                trainX.append(a)\n",
    "                trainY.append(a1[seq_len : seq_len + pre_len])\n",
    "            for i in range(len(test_data) - seq_len -pre_len):\n",
    "                b1 = test_data[i: i + seq_len + pre_len]\n",
    "                b2 = sz_weather_nor_test[i: i + seq_len + pre_len]\n",
    "                b = np.row_stack((b1[0:seq_len],b2[0: seq_len + pre_len],sz_poi_nor[:1]))\n",
    "                testX.append(b)\n",
    "                testY.append(b1[seq_len : seq_len + pre_len])\n",
    "\n",
    "\n",
    "    trainX1 = np.array(trainX)\n",
    "    trainY1 = np.array(trainY)\n",
    "    testX1 = np.array(testX)\n",
    "\n",
    "    testY1 = np.array(testY)\n",
    "    print(trainX1.shape)\n",
    "    print(trainY1.shape)\n",
    "    print(testX1.shape)\n",
    "    print(testY1.shape)\n",
    "    \n",
    "    return trainX1, trainY1, testX1, testY1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ef25ca9-de68-40dd-9723-7939696d707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_len = data.shape[0]\n",
    "num_nodes = data.shape[1]\n",
    "data1 =np.mat(data,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d2d6032-04b9-47bc-a4d1-f7db91ddc79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2976\n",
      "156\n",
      "(2976, 156)\n"
     ]
    }
   ],
   "source": [
    "print(time_len)\n",
    "print(num_nodes)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72d32c21-215e-479f-b2cd-c9874747490e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>90217</th>\n",
       "      <th>90218</th>\n",
       "      <th>90219</th>\n",
       "      <th>90220</th>\n",
       "      <th>90221</th>\n",
       "      <th>90222</th>\n",
       "      <th>90223</th>\n",
       "      <th>90224</th>\n",
       "      <th>90225</th>\n",
       "      <th>90226</th>\n",
       "      <th>...</th>\n",
       "      <th>112746</th>\n",
       "      <th>116097</th>\n",
       "      <th>116098</th>\n",
       "      <th>116099</th>\n",
       "      <th>116100</th>\n",
       "      <th>116157</th>\n",
       "      <th>116219</th>\n",
       "      <th>116220</th>\n",
       "      <th>116221</th>\n",
       "      <th>117189</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.471971</td>\n",
       "      <td>18.455410</td>\n",
       "      <td>20.590635</td>\n",
       "      <td>15.345258</td>\n",
       "      <td>9.585218</td>\n",
       "      <td>21.501821</td>\n",
       "      <td>31.611759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.008941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.623514</td>\n",
       "      <td>31.126936</td>\n",
       "      <td>45.680952</td>\n",
       "      <td>18.801449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.791096</td>\n",
       "      <td>16.589562</td>\n",
       "      <td>32.590224</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.807137</td>\n",
       "      <td>15.713816</td>\n",
       "      <td>27.523695</td>\n",
       "      <td>11.087895</td>\n",
       "      <td>9.455280</td>\n",
       "      <td>17.332246</td>\n",
       "      <td>33.976531</td>\n",
       "      <td>12.968823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.921757</td>\n",
       "      <td>...</td>\n",
       "      <td>4.193466</td>\n",
       "      <td>14.889713</td>\n",
       "      <td>29.400996</td>\n",
       "      <td>47.128457</td>\n",
       "      <td>18.081014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.492316</td>\n",
       "      <td>39.614822</td>\n",
       "      <td>32.588299</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.809457</td>\n",
       "      <td>8.979647</td>\n",
       "      <td>20.280394</td>\n",
       "      <td>16.523419</td>\n",
       "      <td>8.003314</td>\n",
       "      <td>15.789483</td>\n",
       "      <td>13.747267</td>\n",
       "      <td>12.221143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.588447</td>\n",
       "      <td>...</td>\n",
       "      <td>11.076185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.198482</td>\n",
       "      <td>44.345346</td>\n",
       "      <td>17.947455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.289410</td>\n",
       "      <td>33.331948</td>\n",
       "      <td>37.471168</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51.590372</td>\n",
       "      <td>23.631243</td>\n",
       "      <td>20.224094</td>\n",
       "      <td>15.116459</td>\n",
       "      <td>6.642644</td>\n",
       "      <td>17.575806</td>\n",
       "      <td>17.657556</td>\n",
       "      <td>15.998745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.228905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.132564</td>\n",
       "      <td>60.259055</td>\n",
       "      <td>57.122521</td>\n",
       "      <td>35.479929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.244356</td>\n",
       "      <td>12.942474</td>\n",
       "      <td>47.140562</td>\n",
       "      <td>39.697219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.770433</td>\n",
       "      <td>20.437740</td>\n",
       "      <td>20.465606</td>\n",
       "      <td>14.820217</td>\n",
       "      <td>11.344404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.878502</td>\n",
       "      <td>5.676248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.250064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.904281</td>\n",
       "      <td>66.134278</td>\n",
       "      <td>66.451325</td>\n",
       "      <td>37.001600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.958725</td>\n",
       "      <td>38.225794</td>\n",
       "      <td>39.550687</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       90217      90218      90219      90220      90221      90222  \\\n",
       "0   8.471971  18.455410  20.590635  15.345258   9.585218  21.501821   \n",
       "1   7.807137  15.713816  27.523695  11.087895   9.455280  17.332246   \n",
       "2   8.809457   8.979647  20.280394  16.523419   8.003314  15.789483   \n",
       "3  51.590372  23.631243  20.224094  15.116459   6.642644  17.575806   \n",
       "4  58.770433  20.437740  20.465606  14.820217  11.344404   0.000000   \n",
       "\n",
       "       90223      90224  90225      90226  ...     112746     116097  \\\n",
       "0  31.611759   0.000000    0.0  22.008941  ...   0.000000  27.623514   \n",
       "1  33.976531  12.968823    0.0  17.921757  ...   4.193466  14.889713   \n",
       "2  13.747267  12.221143    0.0  21.588447  ...  11.076185   0.000000   \n",
       "3  17.657556  15.998745    0.0  24.228905  ...   0.000000  34.132564   \n",
       "4  33.878502   5.676248    0.0  26.250064  ...   0.000000  15.904281   \n",
       "\n",
       "      116098     116099     116100  116157     116219     116220     116221  \\\n",
       "0  31.126936  45.680952  18.801449     0.0  22.791096  16.589562  32.590224   \n",
       "1  29.400996  47.128457  18.081014     0.0  22.492316  39.614822  32.588299   \n",
       "2  30.198482  44.345346  17.947455     0.0  22.289410  33.331948  37.471168   \n",
       "3  60.259055  57.122521  35.479929     0.0  40.244356  12.942474  47.140562   \n",
       "4  66.134278  66.451325  37.001600     0.0  37.958725  38.225794  39.550687   \n",
       "\n",
       "      117189  \n",
       "0   0.000000  \n",
       "1   0.000000  \n",
       "2   0.000000  \n",
       "3  39.697219  \n",
       "4   0.000000  \n",
       "\n",
       "[5 rows x 156 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab0a0192-b112-484c-9ff6-2e56bce9c6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2976, 156)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f44d03e1-afdb-4d87-9fea-70df6d070876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2976, 156)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz_weather = pd.read_csv('../DAT/sz_weather.csv',header = None)\n",
    "sz_weather = np.mat(sz_weather)\n",
    "sz_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a780adb-e810-4384-85f2-349662ee5c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.0980221 , 0.21353212, 0.23823702, ..., 0.19194394, 0.37707424,\n",
       "         0.        ],\n",
       "        [0.09032986, 0.18181142, 0.31845367, ..., 0.45834997, 0.37705195,\n",
       "         0.        ],\n",
       "        [0.10192686, 0.10389599, 0.23464748, ..., 0.3856561 , 0.43354756,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.37947276, 0.141638  , 0.10453826, ..., 0.13605069, 0.1998609 ,\n",
       "         0.        ],\n",
       "        [0.39722532, 0.18600048, 0.121989  , ..., 0.16306393, 0.16957766,\n",
       "         0.        ],\n",
       "        [0.38101357, 0.14001252, 0.10420388, ..., 0.19054307, 0.1845446 ,\n",
       "         0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "907c4a16-c392-4477-aafd-48e8b75821e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2, 2, 2, ..., 2, 2, 2],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 2, 2, ..., 2, 2, 2]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11ae8591-3f52-49c6-a570-96ed9d24132c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2976, 156)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3244de6e-7353-4e2c-931e-b38396e0c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.row_stack((data1, sz_weather))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "292c03d5-9dcb-4cf6-8c18-79234e56a652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5952, 156)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02c96cfe-18a8-4a37-ae07-e7f151be2388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.0980221 , 0.21353212, 0.23823702, ..., 0.19194394, 0.37707424,\n",
       "         0.        ],\n",
       "        [0.09032986, 0.18181142, 0.31845367, ..., 0.45834997, 0.37705195,\n",
       "         0.        ],\n",
       "        [0.10192686, 0.10389599, 0.23464748, ..., 0.38565609, 0.43354756,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [2.        , 2.        , 2.        , ..., 2.        , 2.        ,\n",
       "         2.        ],\n",
       "        [2.        , 2.        , 2.        , ..., 2.        , 2.        ,\n",
       "         2.        ],\n",
       "        [2.        , 2.        , 2.        , ..., 2.        , 2.        ,\n",
       "         2.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb7ee153-079e-4d6a-9585-e1f625be8957",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m data1  \u001b[38;5;241m=\u001b[39m data1\u001b[38;5;241m/\u001b[39mmax_value\n\u001b[1;32m      5\u001b[0m data1\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel_name\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mast-gcn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      8\u001b[0m         name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd poi dim\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "#### normalization\n",
    "max_value = np.max(data1)\n",
    "data1  = data1/max_value\n",
    "\n",
    "data1.columns = data.columns\n",
    "if model_name == 'ast-gcn':\n",
    "    if scheme == 1:\n",
    "        name = 'add poi dim'\n",
    "    elif scheme == 2:\n",
    "        name = 'add weather dim'\n",
    "    else:\n",
    "        name = 'add poi + weather dim'\n",
    "\n",
    "else:\n",
    "    name = 'tgcn'\n",
    "\n",
    "print('model:', model_name)\n",
    "print('scheme:', name)\n",
    "print('noise_name:', noise_name)\n",
    "print('noise_param:', PG)\n",
    "\n",
    "trainX, trainY, testX, testY = preprocess_data(data1, time_len, train_rate, seq_len, pre_len, model_name, scheme)\n",
    "\n",
    "totalbatch = int(trainX.shape[0]/batch_size)\n",
    "training_data_count = len(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb53f9f2-b319-4fa0-8dd8-537a88e13cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit():\n",
    "    def __init__(self, dim, num_nodes, reuse = None):\n",
    "#        super(Unit, self).__init__(_reuse=reuse)\n",
    "        self.dim = dim\n",
    "        self.num_nodes = num_nodes\n",
    "    def call(self, inputs, time_len):\n",
    "        x, e = inputs  \n",
    "        unit_matrix1 = tf.matmul(x, e)\n",
    "        unit_matrix = tf.convert_to_tensor(unit_matrix1)\n",
    "        self.weight_unit ,self.bias_unit = self._emb(dim, time_len)\n",
    "        \n",
    "        x1 = tf.matmul(tf.cast(unit_matrix,tf.float32),self.weight_unit)\n",
    "        x_output = tf.add(x1, self.bias_unit)\n",
    "#        x_output = pd.DataFrame(x_output)\n",
    "#        x_output = tf.reshape(x_output, shape=[-1, self.num_nodes, time_len])\n",
    "#        x_output = tf.reshape(x_output, shape=[-1, self.num_nodes * time_len])\n",
    "        return x_output\n",
    "    \n",
    "    def _emb(self, dim, time_len):\n",
    "        \n",
    "        with tf.variable_scope('a',reuse = tf.AUTO_REUSE):\n",
    "            weight_unit = tf.get_variable(name = 'weight_unit', shape = (self.dim, self.num_nodes), dtype = tf.float32)\n",
    "            bias_unit = tf.get_variable(name = 'bias_unit', shape =(time_len,1), initializer = tf.constant_initializer(dtype=tf.float32))\n",
    "        w = weight_unit\n",
    "        b = bias_unit\n",
    "        return w, b\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea62c9e2-3651-4698-a3ad-1383cba77f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit1():\n",
    "    def __init__(self, dim, num_nodes, reuse = None):\n",
    "#        super(Unit, self).__init__(_reuse=reuse)\n",
    "        self.dim = dim\n",
    "        self.num_nodes = num_nodes\n",
    "    def call(self, inputs, time_len):\n",
    "        x, e = inputs  \n",
    "        unit_matrix1 = tf.matmul(x, e)\n",
    "        unit_matrix=tf.convert_to_tensor(unit_matrix1)\n",
    "        self.weight_unit1 ,self.bias_unit1 = self._emb(dim, time_len)\n",
    "        \n",
    "        x1=tf.matmul(tf.cast(unit_matrix,tf.float32),self.weight_unit1)\n",
    "        x_output= tf.add(x1, self.bias_unit1)\n",
    "#        x_output = pd.DataFrame(x_output)\n",
    "#        x_output = tf.reshape(x_output, shape=[-1, self.num_nodes, time_len])\n",
    "#        x_output = tf.reshape(x_output, shape=[-1, self.num_nodes * time_len])\n",
    "        return x_output\n",
    "    \n",
    "    def _emb(self, dim, time_len):\n",
    "        \n",
    "        with tf.variable_scope('a',reuse = tf.AUTO_REUSE):\n",
    "            weight_unit1 = tf.get_variable(name = 'weight_unit1', shape = (self.dim, self.num_nodes), dtype = tf.float32)\n",
    "            bias_unit1 = tf.get_variable(name = 'bias_unit1', shape =(time_len,1), initializer = tf.constant_initializer(dtype=tf.float32))\n",
    "        w1 = weight_unit1\n",
    "        bb = bias_unit1\n",
    "        return w1, bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84da73f9-2655-472b-a0f5-b9a180a360bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit2():\n",
    "    def __init__(self, dim, num_nodes, time_len, reuse = None):\n",
    "#        super(Unit, self).__init__(_reuse=reuse)\n",
    "        self.dim = dim\n",
    "        self.num_nodes = num_nodes\n",
    "        self.time_len = time_len\n",
    "    def call(self, inputs, time_len):\n",
    "        x, e = inputs\n",
    "        x = np.transpose(x)\n",
    "        x = x.astype(np.float64)\n",
    "        unit_matrix1 = tf.matmul(x, e)\n",
    "        unit_matrix = tf.convert_to_tensor(unit_matrix1)\n",
    "        self.weight_unit ,self.bias_unit = self._emb(dim, time_len)\n",
    "        \n",
    "        x1 = tf.matmul(tf.cast(unit_matrix,tf.float32),self.weight_unit)\n",
    "        self.x_output = tf.add(x1, self.bias_unit)\n",
    "#        print(x_output)\n",
    "        self.x_output = tf.transpose(self.x_output)\n",
    "#        x = x.astype(np.float64)\n",
    "#        x_output = pd.DataFrame(x_output)\n",
    "#        x_output = tf.reshape(x_output, shape=[-1, self.num_nodes, time_len])\n",
    "#        x_output = tf.reshape(x_output, shape=[-1, self.num_nodes * time_len])\n",
    "        return self.x_output\n",
    "    \n",
    "    def _emb(self, dim, time_len):\n",
    "        \n",
    "        with tf.variable_scope('a',reuse = tf.AUTO_REUSE):\n",
    "            self.weight_unit = tf.get_variable(name = 'weight_unit', shape = (self.dim, self.time_len), dtype = tf.float32)\n",
    "            self.bias_unit = tf.get_variable(name = 'bias_unit', shape =(num_nodes,1), initializer = tf.constant_initializer(dtype=tf.float32))\n",
    "        self.w = self.weight_unit\n",
    "        self.b = self.bias_unit\n",
    "        return self.w, self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d457e5ef-5b13-42d4-86a2-481039a1b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit3():\n",
    "    def __init__(self, dim, num_nodes, time_len, reuse = None):\n",
    "#        super(Unit, self).__init__(_reuse=reuse)\n",
    "        self.dim = dim\n",
    "        self.num_nodes = num_nodes\n",
    "        self.time_len = time_len\n",
    "    def call(self, inputs, time_len):\n",
    "        x, e = inputs\n",
    "        x = np.transpose(x)\n",
    "        x = x.astype(np.float64)\n",
    "        unit_matrix1 = tf.matmul(x, e)\n",
    "        unit_matrix = tf.convert_to_tensor(unit_matrix1)\n",
    "        self.weight_unit ,self.bias_unit = self._emb(dim, time_len)\n",
    "        \n",
    "        x1 = tf.matmul(tf.cast(unit_matrix,tf.float32),self.weight_unit)\n",
    "        x_output = tf.add(x1, self.bias_unit)\n",
    "        x_output = tf.transpose(x_output)\n",
    "#        x_output = pd.DataFrame(x_output)\n",
    "#        x_output = tf.reshape(x_output, shape=[-1, self.num_nodes, time_len])\n",
    "#        x_output = tf.reshape(x_output, shape=[-1, self.num_nodes * time_len])\n",
    "        return x_output\n",
    "    \n",
    "    def _emb(self, dim, time_len):\n",
    "        \n",
    "        with tf.variable_scope('a',reuse = tf.AUTO_REUSE):\n",
    "            weight_unit = tf.get_variable(name = 'weight_unit', shape = (self.dim, self.time_len), dtype = tf.float32)\n",
    "            bias_unit = tf.get_variable(name = 'bias_unit', shape =(num_nodes,1), initializer = tf.constant_initializer(dtype=tf.float32))\n",
    "        w = weight_unit\n",
    "        b = bias_unit\n",
    "        return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a172559-7e94-44ad-a7a9-527fb5f1c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit4():\n",
    "    def __init__(self, dim, num_nodes, reuse = None):\n",
    "#        super(Unit, self).__init__(_reuse=reuse)\n",
    "        self.dim = dim\n",
    "        self.num_nodes = num_nodes\n",
    "    def call(self, inputs, time_len):\n",
    "        x, e = inputs  \n",
    "        unit_matrix1 = tf.matmul(x, e)\n",
    "        unit_matrix = tf.convert_to_tensor(unit_matrix1)\n",
    "        self.weight_unit ,self.bias_unit = self._emb(dim, time_len)\n",
    "        \n",
    "        x1 = tf.matmul(tf.cast(unit_matrix,tf.float32),self.weight_unit)\n",
    "        x_output = tf.add(x1, self.bias_unit)\n",
    "#        x_output = pd.DataFrame(x_output)\n",
    "#        x_output = tf.reshape(x_output, shape=[-1, self.num_nodes, time_len])\n",
    "#        x_output = tf.reshape(x_output, shape=[-1, self.num_nodes * time_len])\n",
    "        return x_output\n",
    "    \n",
    "    def _emb(self, dim, time_len):\n",
    "        \n",
    "        with tf.variable_scope('a',reuse = tf.AUTO_REUSE):\n",
    "            weight_unit = tf.get_variable(name = 'weight_unit', shape = (self.dim, self.num_nodes), dtype = tf.float32)\n",
    "            bias_unit = tf.get_variable(name = 'bias_unit', shape =(time_len,1), initializer = tf.constant_initializer(dtype=tf.float32))\n",
    "        w = weight_unit\n",
    "        b = bias_unit\n",
    "        return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01f3c7b3-8be0-4d35-a52b-e91efda855e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit5():\n",
    "    def __init__(self, dim, num_nodes, reuse = None):\n",
    "#        super(Unit, self).__init__(_reuse=reuse)\n",
    "        self.dim = dim\n",
    "        self.num_nodes = num_nodes\n",
    "    def call(self, inputs, time_len):\n",
    "        x, e = inputs  \n",
    "        unit_matrix1 = tf.matmul(x, e)\n",
    "        unit_matrix=tf.convert_to_tensor(unit_matrix1)\n",
    "        self.weight_unit1 ,self.bias_unit1 = self._emb(dim, time_len)\n",
    "        \n",
    "        x1=tf.matmul(tf.cast(unit_matrix,tf.float32),self.weight_unit1)\n",
    "        self.x_output= tf.add(x1, self.bias_unit1)\n",
    "#        x_output = pd.DataFrame(x_output)\n",
    "#        x_output = tf.reshape(x_output, shape=[-1, self.num_nodes, time_len])\n",
    "#        x_output = tf.reshape(x_output, shape=[-1, self.num_nodes * time_len])\n",
    "        return self.x_output\n",
    "    \n",
    "    def _emb(self, dim, time_len):\n",
    "        \n",
    "        with tf.variable_scope('a',reuse = tf.AUTO_REUSE):\n",
    "            self.weight_unit1 = tf.get_variable(name = 'weight_unit1', shape = (self.dim, self.num_nodes), dtype = tf.float32)\n",
    "            self.bias_unit1 = tf.get_variable(name = 'bias_unit1', shape =(time_len,1), initializer = tf.constant_initializer(dtype=tf.float32))\n",
    "        self.w1 = self.weight_unit1\n",
    "        self.bb = self.bias_unit1\n",
    "        return self.w1, self.bb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d64721-7bff-4760-9050-f0e21d91d505",
   "metadata": {},
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d2efd06-945b-4ebf-9e88-4867da7c061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import scipy.sparse as sp\n",
    "#from scipy.sparse import linalg\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalized_adj(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    normalized_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "    normalized_adj = normalized_adj.astype(np.float32)\n",
    "    return normalized_adj\n",
    "    \n",
    "def sparse_to_tuple(mx):\n",
    "    mx = mx.tocoo()\n",
    "    coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "    L = tf.SparseTensor(coords, mx.data, mx.shape)\n",
    "    return tf.sparse_reorder(L) \n",
    "    \n",
    "def calculate_laplacian(adj, lambda_max=1):  \n",
    "    adj = normalized_adj(adj + sp.eye(adj.shape[0]))\n",
    "    adj = sp.csr_matrix(adj)\n",
    "    adj = adj.astype(np.float32)\n",
    "    return sparse_to_tuple(adj)\n",
    "    \n",
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
    "                            maxval=init_range, dtype=tf.float32)\n",
    "\n",
    "    return tf.Variable(initial,name=name)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36bf0df-c2a8-42eb-9fa7-9ebfe40992d8",
   "metadata": {},
   "source": [
    "# tgcn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dceaeb5e-48b6-4302-82d9-c9e88636d0c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/49/df/c6e5ef56bed7c621763867f38aabd860aa55d7da27bccfc72f4816d9969f/tensorflow-2.18.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.18.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=24.3.25 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (4.24.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (1.58.0)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.19,>=2.18 from https://files.pythonhosted.org/packages/b1/de/021c1d407befb505791764ad2cbd56ceaaa53a746baed01d2e2143f05f18/tensorboard-2.18.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Obtaining dependency information for keras>=3.5.0 from https://files.pythonhosted.org/packages/c2/88/eef50051a772dcb4433d1f3e4c1d6576ba450fe83e89d028d7e8b85a2122/keras-3.6.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
      "  Obtaining dependency information for numpy<2.1.0,>=1.26.0 from https://files.pythonhosted.org/packages/b9/14/78635daab4b07c0930c919d451b8bf8c164774e6a3413aed04a6d95758ce/numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=3.11.0 (from tensorflow)\n",
      "  Obtaining dependency information for h5py>=3.11.0 from https://files.pythonhosted.org/packages/8e/1d/631c200e6d5d067035c58028f305cf7f29c494ddfb9b9484a907a367c8bd/h5py-3.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading h5py-3.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes<0.5.0,>=0.4.0 from https://files.pythonhosted.org/packages/3e/55/b9711de47135d4d8766ff7907fe54c8bffff545fd646817c352de37b0ad5/ml_dtypes-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading ml_dtypes-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for rich from https://files.pythonhosted.org/packages/9a/e2/10e9819cf4a20bd8ea2f5dabafc2e6bf4a78d6a0965daeb60a4b34d1c11f/rich-13.9.3-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.9.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for namex from https://files.pythonhosted.org/packages/73/59/7854fbfb59f8ae35483ce93493708be5942ebb6328cd85b3a609df629736/namex-0.0.8-py3-none-any.whl.metadata\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for optree from https://files.pythonhosted.org/packages/c9/49/db3a3f9ef897291cd53151985eb569c008292f53f037d98317e6d1f61739/optree-0.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading optree-0.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py39/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py39/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py39/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py39/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/envs/py39/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/envs/py39/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/envs/py39/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/py39/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/py39/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow) (3.17.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for mdurl~=0.1 from https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl.metadata\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.18.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading h5py-3.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.8/358.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.3-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, flatbuffers, optree, numpy, mdurl, ml-dtypes, markdown-it-py, h5py, tensorboard, rich, keras, tensorflow\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 23.5.26\n",
      "    Uninstalling flatbuffers-23.5.26:\n",
      "      Successfully uninstalled flatbuffers-23.5.26\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.13.0\n",
      "    Uninstalling tensorboard-2.13.0:\n",
      "      Successfully uninstalled tensorboard-2.13.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.13.1\n",
      "    Uninstalling keras-2.13.1:\n",
      "      Successfully uninstalled keras-2.13.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.1.1 requires numpy<2.0,>=1.16; python_version <= \"3.11\", but you have numpy 2.0.2 which is incompatible.\n",
      "cupy-cuda11x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
      "langchain 0.0.295 requires numpy<2,>=1, but you have numpy 2.0.2 which is incompatible.\n",
      "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
      "scipy 1.11.2 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.0.2 which is incompatible.\n",
      "torchvision 0.15.2 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed flatbuffers-24.3.25 h5py-3.12.1 keras-3.6.0 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 optree-0.13.0 rich-13.9.3 tensorboard-2.18.0 tensorflow-2.18.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a327c7cf-ae3a-442a-9eb0-c5b662797fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.contrib.rnn import RNNCell\n",
    "from tensorflow.python.keras.layers import GRUCell, RNN\n",
    "\n",
    "# from utils import calculate_laplacian\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6af9a-3111-4750-98b0-4a334576cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tgcnCell(tf.keras.layers.Layer):\n",
    "    \"\"\"Temporal Graph Convolutional Network \"\"\"\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, num_units, adj, num_nodes, input_size=None,\n",
    "                 act=tf.nn.tanh, reuse=None):\n",
    "\n",
    "        super(tgcnCell, self).__init__(_reuse=reuse)\n",
    "        self._act = act\n",
    "        self._nodes = num_nodes\n",
    "        self._units = num_units\n",
    "        self._adj = []\n",
    "        self._adj.append(calculate_laplacian(adj))\n",
    "\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._nodes * self._units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "\n",
    "        with tf.variable_scope(scope or \"tgcn\"):\n",
    "            with tf.variable_scope(\"gates\"):  \n",
    "                value = tf.nn.sigmoid(\n",
    "                    self._gc(inputs, state, 2 * self._units, bias=1.0, scope=scope))\n",
    "                r, u = tf.split(value=value, num_or_size_splits=2, axis=1)\n",
    "            with tf.variable_scope(\"candidate\"):\n",
    "                r_state = r * state\n",
    "                c = self._act(self._gc(inputs, r_state, self._units, scope=scope))\n",
    "            new_h = u * state + (1 - u) * c\n",
    "        return new_h, new_h\n",
    "\n",
    "\n",
    "    def _gc(self, inputs, state, output_size, bias=0.0, scope=None):\n",
    "        ## inputs:(-1,num_nodes)\n",
    "        inputs = tf.expand_dims(inputs, 2)\n",
    "#        print('inputs_shape:',inputs.shape)\n",
    "        ## state:(batch,num_node,gru_units)\n",
    "        state = tf.reshape(state, (-1, self._nodes, self._units))\n",
    "#        print('state_shape:',state.shape)\n",
    "        ## concat\n",
    "        x_s = tf.concat([inputs, state], axis=2)\n",
    "#        print('x_s_shape:',x_s.shape)\n",
    "        \n",
    "        \n",
    "#        kgembedding = np.array(pd.read_csv(r'/DHH/sz_gcn/sz_data/sz_poi_transR_embedding20.csv',header=None))\n",
    "#        kgeMatrix = np.repeat(kgembedding[np.newaxis, :, :], self._units, axis=0)\n",
    "#        kgeMatrix = tf.reshape(tf.constant(kgeMatrix, dtype=tf.float32), (self._units, -1))\n",
    "#        kgMatrix = tf.reshape(kgeMatrix,(-1,self._nodes, 20))\n",
    "#        \n",
    "#        ## inputs:(-1,num_nodes)\n",
    "#        inputs = tf.expand_dims(inputs, 2)\n",
    "#        ## state:(batch,num_node,gru_units)\n",
    "#        state = tf.reshape(state, (-1, self._nodes, self._units))\n",
    "#        ## concat\n",
    "#        print('kgMatrix_shape:',kgMatrix.shape)\n",
    "#        print('inputs_shape:',inputs.shape)\n",
    "#        print('state_shape:',state.shape)\n",
    "#        kg_x = tf.concat([inputs, kgMatrix],axis = 2)\n",
    "#        print('kg_x_shape:',kg_x.shape)\n",
    "#        x_s = tf.concat([kg_x, state], axis=2)\n",
    "        input_size = x_s.get_shape()[2].value\n",
    "        ## (num_node,input_size,-1)\n",
    "        x0 = tf.transpose(x_s, perm=[1, 2, 0])  \n",
    "        x0 = tf.reshape(x0, shape=[self._nodes, -1])\n",
    "        \n",
    "        scope = tf.get_variable_scope()\n",
    "        with tf.variable_scope(scope):\n",
    "            for m in self._adj:\n",
    "                x1 = tf.sparse_tensor_dense_matmul(m, x0)\n",
    "#                print(x1)\n",
    "            x = tf.reshape(x1, shape=[self._nodes, input_size,-1])\n",
    "            x = tf.transpose(x,perm=[2,0,1])\n",
    "            x = tf.reshape(x, shape=[-1, input_size])\n",
    "            weights = tf.get_variable(\n",
    "                'weights', [input_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            x = tf.matmul(x, weights)  # (batch_size * self._nodes, output_size)\n",
    "            biases = tf.get_variable(\n",
    "                \"biases\", [output_size], initializer=tf.constant_initializer(bias, dtype=tf.float32))\n",
    "            x = tf.nn.bias_add(x, biases)\n",
    "            x = tf.reshape(x, shape=[-1, self._nodes, output_size])\n",
    "            x = tf.reshape(x, shape=[-1, self._nodes * output_size])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca1750-5ed6-4bc9-920c-cb885d46fdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48529984-892b-46f5-8251-7f4054cbdcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import numpy.linalg as la\n",
    "from acell import preprocess_data,load_assist_data\n",
    "from tgcn import tgcnCell\n",
    "\n",
    "from visualization import plot_result,plot_error\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "\n",
    "time_start = time.time()\n",
    "###### Settings ######\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('training_epoch', 3000, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('gru_units', 128, 'hidden units of gru.')\n",
    "flags.DEFINE_integer('seq_len',10 , 'time length of inputs.')\n",
    "flags.DEFINE_integer('pre_len', 3, 'time length of prediction.')\n",
    "flags.DEFINE_float('train_rate', 0.8, 'rate of training set.')\n",
    "flags.DEFINE_integer('batch_size', 64, 'batch size.')\n",
    "flags.DEFINE_string('dataset', 'sz', 'dataset')\n",
    "flags.DEFINE_string('model_name', 'tgcn', 'ast-gcn')\n",
    "flags.DEFINE_string('scheme', 1, 'scheme')\n",
    "flags.DEFINE_string('noise_name', 'None', 'None or Gauss or Possion')\n",
    "flags.DEFINE_string('noise_param', 0, 'Parameter for noise')\n",
    "\n",
    "model_name = FLAGS.model_name\n",
    "noise_name = FLAGS.noise_name\n",
    "data_name = FLAGS.dataset\n",
    "train_rate =  FLAGS.train_rate\n",
    "seq_len = FLAGS.seq_len\n",
    "output_dim = pre_len = FLAGS.pre_len\n",
    "batch_size = FLAGS.batch_size\n",
    "lr = FLAGS.learning_rate\n",
    "training_epoch = FLAGS.training_epoch\n",
    "gru_units = FLAGS.gru_units\n",
    "dim = FLAGS.dim\n",
    "scheme = FLAGS.scheme\n",
    "PG = FLAGS.noise_param\n",
    "\n",
    "###### load data ######\n",
    "if data_name == 'sz':\n",
    "    data, adj = load_assist_data('sz')\n",
    "#if data_name == 'sh':\n",
    "#    data, adj = load_sh_data('sh')\n",
    "### Perturbation Analysis\n",
    "def MaxMinNormalization(x,Max,Min):\n",
    "    x = (x-Min)/(Max-Min)\n",
    "    return x\n",
    "\n",
    "if noise_name == 'Gauss':\n",
    "    Gauss = np.random.normal(0,PG,size=data.shape)\n",
    "    noise_Gauss = MaxMinNormalization(Gauss,np.max(Gauss),np.min(Gauss))\n",
    "    data = data + noise_Gauss\n",
    "elif noise_name == 'Possion':\n",
    "    Possion = np.random.poisson(PG,size=data.shape)\n",
    "    noise_Possion = MaxMinNormalization(Possion,np.max(Possion),np.min(Possion))\n",
    "    data = data + noise_Possion\n",
    "else:\n",
    "    data = data\n",
    "\n",
    "time_len = data.shape[0]\n",
    "num_nodes = data.shape[1]\n",
    "data1 =np.mat(data,dtype=np.float32)\n",
    "\n",
    "#### normalization\n",
    "max_value = np.max(data1)\n",
    "data1  = data1/max_value\n",
    "\n",
    "data1.columns = data.columns\n",
    "if model_name == 'ast-gcn':\n",
    "    if scheme == 1:\n",
    "        name = 'add poi dim'\n",
    "    elif scheme == 2:\n",
    "        name = 'add weather dim'\n",
    "    else:\n",
    "        name = 'add poi + weather dim'\n",
    "\n",
    "else:\n",
    "    name = 'tgcn'\n",
    "\n",
    "print('model:', model_name)\n",
    "print('scheme:', name)\n",
    "print('noise_name:', noise_name)\n",
    "print('noise_param:', PG)\n",
    "\n",
    "trainX, trainY, testX, testY = preprocess_data(data1, time_len, train_rate, seq_len, pre_len, model_name, scheme)\n",
    "\n",
    "totalbatch = int(trainX.shape[0]/batch_size)\n",
    "training_data_count = len(trainX)\n",
    "\n",
    "def TGCN(_X, _weights, _biases):\n",
    "    ###\n",
    "    cell_1 = tgcnCell(gru_units, adj, num_nodes=num_nodes)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True)\n",
    "    _X = tf.unstack(_X, axis=1)\n",
    "    outputs, states = tf.nn.static_rnn(cell, _X, dtype=tf.float32)\n",
    "    m = []\n",
    "    for i in outputs:\n",
    "        o = tf.reshape(i,shape=[-1,num_nodes,gru_units])\n",
    "        o = tf.reshape(o,shape=[-1,gru_units])\n",
    "        m.append(o)\n",
    "    last_output = m[-1]\n",
    "    output = tf.matmul(last_output, _weights['out']) + _biases['out']\n",
    "    output = tf.reshape(output,shape=[-1,num_nodes,pre_len])\n",
    "    output = tf.transpose(output, perm=[0,2,1])\n",
    "    output = tf.reshape(output, shape=[-1,num_nodes])\n",
    "    return output, m, states\n",
    "    \n",
    "    \n",
    "###### placeholders ######\n",
    "if model_name == 'ast-gcn':\n",
    "    if scheme == 1:\n",
    "        inputs = tf.placeholder(tf.float32, shape=[None, seq_len+1, num_nodes])\n",
    "    elif scheme == 2:\n",
    "        inputs = tf.placeholder(tf.float32, shape=[None, seq_len*2+pre_len, num_nodes])\n",
    "    else:\n",
    "        inputs = tf.placeholder(tf.float32, shape=[None, seq_len*2+pre_len+1, num_nodes])\n",
    "\n",
    "else:\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, seq_len, num_nodes])\n",
    "\n",
    "labels = tf.placeholder(tf.float32, shape=[None, pre_len, num_nodes])\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([gru_units, pre_len], mean=1.0), name='weight_o')}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([pre_len]),name='bias_o')}\n",
    "\n",
    "pred,ttts,ttto = TGCN(inputs, weights, biases)\n",
    "\n",
    "y_pred = pred\n",
    "      \n",
    "\n",
    "###### optimizer ######\n",
    "lambda_loss = 0.0015\n",
    "Lreg = lambda_loss * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "label = tf.reshape(labels, [-1,num_nodes])\n",
    "##loss\n",
    "print('y_pred_shape:', y_pred.shape)\n",
    "print('label_shape:', label.shape)\n",
    "loss = tf.reduce_mean(tf.nn.l2_loss(y_pred-label) + Lreg)\n",
    "##rmse\n",
    "error = tf.sqrt(tf.reduce_mean(tf.square(y_pred-label)))\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "###### Initialize session ######\n",
    "variables = tf.global_variables()\n",
    "saver = tf.train.Saver(tf.global_variables())  \n",
    "#sess = tf.Session()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#out = 'out/%s'%(model_name)\n",
    "out = 'out/%s_%s'%(model_name,noise_name)\n",
    "path1 = '%s_%s_%s_lr%r_batch%r_unit%r_seq%r_pre%r_epoch%r_scheme%r_PG%r'%(model_name,name,data_name,lr,batch_size,gru_units,seq_len,pre_len,training_epoch,scheme,PG)\n",
    "path = os.path.join(out,path1)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "###### evaluation ######\n",
    "def evaluation(a,b):\n",
    "    rmse = math.sqrt(mean_squared_error(a,b))\n",
    "    mae = mean_absolute_error(a, b)\n",
    "    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n",
    "    r2 = 1-((a-b)**2).sum()/((a-a.mean())**2).sum()\n",
    "    var = 1-(np.var(a-b))/np.var(a)\n",
    "    return rmse, mae, 1-F_norm, r2, var\n",
    " \n",
    "   \n",
    "x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], []\n",
    "test_loss,test_rmse,test_mae,test_acc,test_r2,test_var,test_pred = [],[],[],[],[],[],[]\n",
    "  \n",
    "for epoch in range(training_epoch):\n",
    "    for m in range(totalbatch):\n",
    "        mini_batch = trainX[m * batch_size : (m+1) * batch_size]\n",
    "        mini_label = trainY[m * batch_size : (m+1) * batch_size]\n",
    "        _, loss1, rmse1, train_output = sess.run([optimizer, loss, error, y_pred],\n",
    "                                                 feed_dict = {inputs:mini_batch, labels:mini_label})\n",
    "        batch_loss.append(loss1)\n",
    "        batch_rmse.append(rmse1 * max_value)\n",
    "\n",
    "     # Test completely at every epoch\n",
    "    loss2, rmse2, test_output = sess.run([loss, error, y_pred],\n",
    "                                         feed_dict = {inputs:testX, labels:testY})\n",
    "\n",
    "    testoutput = np.abs(test_output)\n",
    "    test_label = np.reshape(testY,[-1,num_nodes])\n",
    "    rmse, mae, acc, r2_score, var_score = evaluation(test_label, testoutput)\n",
    "    test_label1 = test_label * max_value\n",
    "    test_output1 = testoutput * max_value\n",
    "    test_loss.append(loss2)\n",
    "    test_rmse.append(rmse * max_value)\n",
    "    test_mae.append(mae * max_value)\n",
    "    test_acc.append(acc)\n",
    "    test_r2.append(r2_score)\n",
    "    test_var.append(var_score)\n",
    "    test_pred.append(test_output1)\n",
    "    \n",
    "    print('Iter:{}'.format(epoch),\n",
    "          'train_rmse:{:.4}'.format(batch_rmse[-1]),\n",
    "          'test_loss:{:.4}'.format(loss2),\n",
    "          'test_rmse:{:.4}'.format(rmse),\n",
    "          'test_acc:{:.4}'.format(acc))\n",
    "    \n",
    "    if (epoch % 500 == 0):        \n",
    "        saver.save(sess, path+'/model_100/ASTGCN_pre_%r'%epoch, global_step = epoch)\n",
    "        \n",
    "time_end = time.time()\n",
    "print(time_end-time_start,'s')\n",
    "\n",
    "############## visualization ###############\n",
    "#x = [i for i in range(training_epoch)]\n",
    "b = int(len(batch_rmse)/totalbatch)\n",
    "batch_rmse1 = [i for i in batch_rmse]\n",
    "train_rmse = [(sum(batch_rmse1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n",
    "batch_loss1 = [i for i in batch_loss]\n",
    "train_loss = [(sum(batch_loss1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n",
    "#test_rmse = [float(i) for i in test_rmse]\n",
    "\n",
    "index = test_rmse.index(np.min(test_rmse))\n",
    "test_result = test_pred[index]\n",
    "var = pd.DataFrame(test_result)\n",
    "var.to_csv(path+'/test_result.csv',index = False,header = False)\n",
    "plot_result(test_result,test_label1,path)\n",
    "plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path)\n",
    "evalution = []\n",
    "evalution.append(np.min(test_rmse))\n",
    "evalution.append(test_mae[index])\n",
    "evalution.append(test_acc[index])\n",
    "evalution.append(test_r2[index])\n",
    "evalution.append(test_var[index])\n",
    "evalution = pd.DataFrame(evalution)\n",
    "evalution.to_csv(path+'/evalution.csv',index=False,header=None)\n",
    "print('model_name:', model_name)\n",
    "print('scheme:', scheme)\n",
    "print('name:', name)\n",
    "print('noise_name:', noise_name)\n",
    "print('PG:', PG)\n",
    "print('min_rmse:%r'%(np.min(test_rmse)),\n",
    "      'min_mae:%r'%(test_mae[index]),\n",
    "      'max_acc:%r'%(test_acc[index]),\n",
    "      'r2:%r'%(test_r2[index]),\n",
    "      'var:%r'%test_var[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269ada6-abe5-4a75-9f18-9e97f809f40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e28795b-d97e-48a0-90c6-0972b62ff23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3bf31-f17b-4c35-bdf3-70c90ae36118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb8a5c-ee95-4774-a06b-bc7179a292c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b5724-b445-4239-861c-c4b03fc604e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10109c8-c387-45c7-9f30-c3cacd840591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc5c90-56ba-4f9a-bb50-3627ded65fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d8f00-33c2-441b-8796-870d9d63a38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8ffcc-d962-49b0-84da-825e9d99f316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56803057-e7ef-48de-81e7-3b7abf7a7d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab9a2f-1569-4149-9de3-735b08385244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d0c67-df41-4034-888f-c3124693f856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe8b31-277f-49ff-994e-8f4a0bb644e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3aabd-fcf2-4fdd-bd82-23d131c71eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d040a2-5069-4895-81b0-189083cd5cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
